WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/recommender.py:13: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.

2021-11-26 22:28:40.648369: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-26 22:28:40.689468: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2021-11-26 22:28:40.690056: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fbf70ab2e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-11-26 22:28:40.690128: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
OMP: Info #155: KMP_AFFINITY: Initial OS proc set respected: 0,16
OMP: Info #216: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #157: KMP_AFFINITY: 2 available OS procs
OMP: Info #158: KMP_AFFINITY: Uniform topology
OMP: Info #287: KMP_AFFINITY: topology layer "LL cache" is equivalent to "socket".
OMP: Info #287: KMP_AFFINITY: topology layer "L3 cache" is equivalent to "socket".
OMP: Info #287: KMP_AFFINITY: topology layer "L2 cache" is equivalent to "core".
OMP: Info #287: KMP_AFFINITY: topology layer "L1 cache" is equivalent to "core".
OMP: Info #192: KMP_AFFINITY: 1 socket x 1 core/socket x 2 threads/core (1 total cores)
OMP: Info #218: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #172: KMP_AFFINITY: OS proc 0 maps to socket 0 core 0 thread 0 
OMP: Info #172: KMP_AFFINITY: OS proc 16 maps to socket 0 core 0 thread 1 
OMP: Info #254: KMP_AFFINITY: pid 1533660 tid 1533660 thread 0 bound to OS proc set 0
2021-11-26 22:28:40.691564: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/tpgr.py:51: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/tpgr.py:87: The name tf.random_normal is deprecated. Please use tf.random.normal instead.

WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/tpgr.py:84: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/clusterusers/ecavenaghi/.conda/envs/rs_survey_tensorflow1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/tpgr.py:45: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

OMP: Info #254: KMP_AFFINITY: pid 1533660 tid 1533682 thread 1 bound to OS proc set 16
OMP: Info #254: KMP_AFFINITY: pid 1533660 tid 1533715 thread 2 bound to OS proc set 0
OMP: Info #254: KMP_AFFINITY: pid 1533660 tid 1533683 thread 3 bound to OS proc set 16
OMP: Info #254: KMP_AFFINITY: pid 1533660 tid 1533716 thread 4 bound to OS proc set 0
rating file: demo_data
alpha: 0.0
log: start pre-training
time: Fri Nov 26 22:28:40 2021
pre-train would cover the trained rnn model

log: graph constructed
time: Fri Nov 26 22:29:25 2021
all zero rmse:0.504, rnn rmse:0.515, l2:11.862
all zero rmse:0.504, rnn rmse:0.518, l2:11.862
all zero rmse:0.506, rnn rmse:0.521, l2:11.862
all zero rmse:0.506, rnn rmse:0.522, l2:11.862
all zero rmse:0.506, rnn rmse:0.523, l2:11.862
all zero rmse:0.505, rnn rmse:0.522, l2:11.862
all zero rmse:0.505, rnn rmse:0.522, l2:11.862
all zero rmse:0.505, rnn rmse:0.522, l2:11.862
all zero rmse:0.505, rnn rmse:0.523, l2:11.862
all zero rmse:0.505, rnn rmse:0.523, l2:11.862
all zero rmse:0.505, rnn rmse:0.523, l2:11.862
all zero rmse:0.505, rnn rmse:0.524, l2:11.862
all zero rmse:0.505, rnn rmse:0.524, l2:11.862
all zero rmse:0.505, rnn rmse:0.525, l2:11.862
all zero rmse:0.505, rnn rmse:0.525, l2:11.862
all zero rmse:0.505, rnn rmse:0.526, l2:11.862
all zero rmse:0.505, rnn rmse:0.526, l2:11.862
all zero rmse:0.505, rnn rmse:0.527, l2:11.862
all zero rmse:0.505, rnn rmse:0.527, l2:11.862
all zero rmse:0.505, rnn rmse:0.528, l2:11.862
all zero rmse:0.505, rnn rmse:0.528, l2:11.862
all zero rmse:0.505, rnn rmse:0.529, l2:11.862
all zero rmse:0.505, rnn rmse:0.530, l2:11.862
all zero rmse:0.505, rnn rmse:0.530, l2:11.862
all zero rmse:0.505, rnn rmse:0.531, l2:11.862
all zero rmse:0.505, rnn rmse:0.532, l2:11.862
all zero rmse:0.505, rnn rmse:0.532, l2:11.862
all zero rmse:0.505, rnn rmse:0.533, l2:11.862
all zero rmse:0.505, rnn rmse:0.534, l2:11.862
all zero rmse:0.505, rnn rmse:0.535, l2:11.862
all zero rmse:0.505, rnn rmse:0.536, l2:11.862
all zero rmse:0.505, rnn rmse:0.537, l2:11.862
all zero rmse:0.505, rnn rmse:0.538, l2:11.862
all zero rmse:0.505, rnn rmse:0.539, l2:11.862
all zero rmse:0.505, rnn rmse:0.540, l2:11.862
all zero rmse:0.505, rnn rmse:0.541, l2:11.862
all zero rmse:0.505, rnn rmse:0.542, l2:11.862
all zero rmse:0.505, rnn rmse:0.544, l2:11.862
all zero rmse:0.505, rnn rmse:0.545, l2:11.862
all zero rmse:0.505, rnn rmse:0.546, l2:11.862
all zero rmse:0.505, rnn rmse:0.548, l2:11.862
all zero rmse:0.505, rnn rmse:0.549, l2:11.862
all zero rmse:0.506, rnn rmse:0.550, l2:11.862
all zero rmse:0.506, rnn rmse:0.552, l2:11.862
all zero rmse:0.506, rnn rmse:0.553, l2:11.862
all zero rmse:0.506, rnn rmse:0.554, l2:11.862
all zero rmse:0.505, rnn rmse:0.556, l2:11.862
all zero rmse:0.505, rnn rmse:0.557, l2:11.862
all zero rmse:0.506, rnn rmse:0.559, l2:11.862
all zero rmse:0.506, rnn rmse:0.560, l2:11.862
all zero rmse:0.506, rnn rmse:0.562, l2:11.862
all zero rmse:0.506, rnn rmse:0.563, l2:11.862
all zero rmse:0.506, rnn rmse:0.565, l2:11.862
all zero rmse:0.506, rnn rmse:0.567, l2:11.862
all zero rmse:0.506, rnn rmse:0.568, l2:11.862
all zero rmse:0.506, rnn rmse:0.570, l2:11.862
all zero rmse:0.506, rnn rmse:0.572, l2:11.862
all zero rmse:0.506, rnn rmse:0.573, l2:11.862
all zero rmse:0.506, rnn rmse:0.575, l2:11.862
all zero rmse:0.506, rnn rmse:0.577, l2:11.862
all zero rmse:0.506, rnn rmse:0.579, l2:11.862
all zero rmse:0.506, rnn rmse:0.581, l2:11.862
all zero rmse:0.506, rnn rmse:0.582, l2:11.862
all zero rmse:0.506, rnn rmse:0.584, l2:11.862
train step  1 over
train step  2 over
train step  3 over
train step  4 over
train step  5 over
all zero rmse:0.504, rnn rmse:0.233, l2:72.585
all zero rmse:0.507, rnn rmse:0.278, l2:72.585
all zero rmse:0.507, rnn rmse:0.309, l2:72.585
all zero rmse:0.508, rnn rmse:0.332, l2:72.585
all zero rmse:0.508, rnn rmse:0.348, l2:72.585
all zero rmse:0.507, rnn rmse:0.361, l2:72.585
all zero rmse:0.507, rnn rmse:0.371, l2:72.585
all zero rmse:0.507, rnn rmse:0.378, l2:72.585
all zero rmse:0.507, rnn rmse:0.384, l2:72.585
all zero rmse:0.507, rnn rmse:0.388, l2:72.585
all zero rmse:0.507, rnn rmse:0.392, l2:72.585
all zero rmse:0.506, rnn rmse:0.395, l2:72.585
all zero rmse:0.506, rnn rmse:0.397, l2:72.585
all zero rmse:0.506, rnn rmse:0.398, l2:72.585
all zero rmse:0.506, rnn rmse:0.399, l2:72.585
all zero rmse:0.506, rnn rmse:0.399, l2:72.585
all zero rmse:0.506, rnn rmse:0.399, l2:72.585
all zero rmse:0.506, rnn rmse:0.398, l2:72.585
all zero rmse:0.506, rnn rmse:0.398, l2:72.585
all zero rmse:0.506, rnn rmse:0.397, l2:72.585
all zero rmse:0.506, rnn rmse:0.396, l2:72.585
all zero rmse:0.506, rnn rmse:0.395, l2:72.585
all zero rmse:0.506, rnn rmse:0.394, l2:72.585
all zero rmse:0.506, rnn rmse:0.392, l2:72.585
all zero rmse:0.506, rnn rmse:0.391, l2:72.585
all zero rmse:0.506, rnn rmse:0.389, l2:72.585
all zero rmse:0.506, rnn rmse:0.387, l2:72.585
all zero rmse:0.506, rnn rmse:0.386, l2:72.585
all zero rmse:0.506, rnn rmse:0.384, l2:72.585
all zero rmse:0.506, rnn rmse:0.382, l2:72.585
all zero rmse:0.506, rnn rmse:0.381, l2:72.585
all zero rmse:0.507, rnn rmse:0.379, l2:72.585
all zero rmse:0.507, rnn rmse:0.376, l2:72.585
all zero rmse:0.507, rnn rmse:0.373, l2:72.585
all zero rmse:0.507, rnn rmse:0.370, l2:72.585
all zero rmse:0.506, rnn rmse:0.367, l2:72.585
all zero rmse:0.506, rnn rmse:0.364, l2:72.585
all zero rmse:0.506, rnn rmse:0.361, l2:72.585
all zero rmse:0.506, rnn rmse:0.359, l2:72.585
all zero rmse:0.506, rnn rmse:0.356, l2:72.585
all zero rmse:0.506, rnn rmse:0.353, l2:72.585
all zero rmse:0.506, rnn rmse:0.351, l2:72.585
all zero rmse:0.506, rnn rmse:0.349, l2:72.585
all zero rmse:0.506, rnn rmse:0.346, l2:72.585
all zero rmse:0.506, rnn rmse:0.344, l2:72.585
all zero rmse:0.506, rnn rmse:0.342, l2:72.585
all zero rmse:0.506, rnn rmse:0.340, l2:72.585
all zero rmse:0.506, rnn rmse:0.338, l2:72.585
all zero rmse:0.506, rnn rmse:0.337, l2:72.585
all zero rmse:0.506, rnn rmse:0.335, l2:72.585
all zero rmse:0.506, rnn rmse:0.334, l2:72.585
all zero rmse:0.506, rnn rmse:0.332, l2:72.585
all zero rmse:0.506, rnn rmse:0.331, l2:72.585
all zero rmse:0.506, rnn rmse:0.330, l2:72.585
all zero rmse:0.506, rnn rmse:0.329, l2:72.585
all zero rmse:0.506, rnn rmse:0.328, l2:72.585
all zero rmse:0.506, rnn rmse:0.327, l2:72.585
all zero rmse:0.506, rnn rmse:0.326, l2:72.585
all zero rmse:0.506, rnn rmse:0.325, l2:72.585
all zero rmse:0.506, rnn rmse:0.325, l2:72.585
all zero rmse:0.506, rnn rmse:0.324, l2:72.585
all zero rmse:0.506, rnn rmse:0.324, l2:72.585
all zero rmse:0.506, rnn rmse:0.324, l2:72.585
all zero rmse:0.506, rnn rmse:0.323, l2:72.585
train step  6 over
train step  7 over
train step  8 over
train step  9 over
train step 10 over
all zero rmse:0.509, rnn rmse:0.238, l2:112.754
all zero rmse:0.508, rnn rmse:0.284, l2:112.754
all zero rmse:0.508, rnn rmse:0.312, l2:112.754
all zero rmse:0.508, rnn rmse:0.336, l2:112.754
all zero rmse:0.507, rnn rmse:0.352, l2:112.754
all zero rmse:0.507, rnn rmse:0.367, l2:112.754
all zero rmse:0.507, rnn rmse:0.378, l2:112.754
all zero rmse:0.507, rnn rmse:0.386, l2:112.754
all zero rmse:0.507, rnn rmse:0.393, l2:112.754
all zero rmse:0.507, rnn rmse:0.398, l2:112.754
all zero rmse:0.506, rnn rmse:0.402, l2:112.754
all zero rmse:0.507, rnn rmse:0.404, l2:112.754
all zero rmse:0.507, rnn rmse:0.406, l2:112.754
all zero rmse:0.507, rnn rmse:0.407, l2:112.754
all zero rmse:0.507, rnn rmse:0.407, l2:112.754
all zero rmse:0.507, rnn rmse:0.407, l2:112.754
all zero rmse:0.507, rnn rmse:0.406, l2:112.754
all zero rmse:0.507, rnn rmse:0.406, l2:112.754
all zero rmse:0.507, rnn rmse:0.405, l2:112.754
all zero rmse:0.507, rnn rmse:0.403, l2:112.754
all zero rmse:0.507, rnn rmse:0.402, l2:112.754
all zero rmse:0.507, rnn rmse:0.400, l2:112.754
all zero rmse:0.507, rnn rmse:0.397, l2:112.754
all zero rmse:0.507, rnn rmse:0.395, l2:112.754
all zero rmse:0.507, rnn rmse:0.393, l2:112.754
all zero rmse:0.507, rnn rmse:0.391, l2:112.754
all zero rmse:0.507, rnn rmse:0.389, l2:112.754
all zero rmse:0.507, rnn rmse:0.386, l2:112.754
all zero rmse:0.507, rnn rmse:0.384, l2:112.754
all zero rmse:0.507, rnn rmse:0.381, l2:112.754
all zero rmse:0.506, rnn rmse:0.379, l2:112.754
all zero rmse:0.506, rnn rmse:0.376, l2:112.754
all zero rmse:0.506, rnn rmse:0.372, l2:112.754
all zero rmse:0.506, rnn rmse:0.368, l2:112.754
all zero rmse:0.506, rnn rmse:0.364, l2:112.754
all zero rmse:0.506, rnn rmse:0.360, l2:112.754
all zero rmse:0.507, rnn rmse:0.356, l2:112.754
all zero rmse:0.506, rnn rmse:0.352, l2:112.754
all zero rmse:0.507, rnn rmse:0.348, l2:112.754
all zero rmse:0.507, rnn rmse:0.344, l2:112.754
all zero rmse:0.507, rnn rmse:0.340, l2:112.754
all zero rmse:0.506, rnn rmse:0.337, l2:112.754
all zero rmse:0.506, rnn rmse:0.333, l2:112.754
all zero rmse:0.506, rnn rmse:0.330, l2:112.754
all zero rmse:0.506, rnn rmse:0.327, l2:112.754
all zero rmse:0.506, rnn rmse:0.324, l2:112.754
all zero rmse:0.506, rnn rmse:0.321, l2:112.754
all zero rmse:0.506, rnn rmse:0.318, l2:112.754
all zero rmse:0.506, rnn rmse:0.315, l2:112.754
all zero rmse:0.506, rnn rmse:0.313, l2:112.754
all zero rmse:0.506, rnn rmse:0.310, l2:112.754
all zero rmse:0.506, rnn rmse:0.308, l2:112.754
all zero rmse:0.506, rnn rmse:0.306, l2:112.754
all zero rmse:0.506, rnn rmse:0.304, l2:112.754
all zero rmse:0.506, rnn rmse:0.302, l2:112.754
all zero rmse:0.506, rnn rmse:0.301, l2:112.754
all zero rmse:0.506, rnn rmse:0.299, l2:112.754
all zero rmse:0.506, rnn rmse:0.298, l2:112.754
all zero rmse:0.506, rnn rmse:0.297, l2:112.754
all zero rmse:0.506, rnn rmse:0.297, l2:112.754
all zero rmse:0.506, rnn rmse:0.296, l2:112.754
all zero rmse:0.506, rnn rmse:0.295, l2:112.754
all zero rmse:0.506, rnn rmse:0.295, l2:112.754
all zero rmse:0.506, rnn rmse:0.295, l2:112.754
train step 11 over
train step 12 over
train step 13 over
train step 14 over
train step 15 over
all zero rmse:0.507, rnn rmse:0.211, l2:124.087
all zero rmse:0.507, rnn rmse:0.265, l2:124.087
all zero rmse:0.508, rnn rmse:0.303, l2:124.087
all zero rmse:0.507, rnn rmse:0.330, l2:124.087
all zero rmse:0.507, rnn rmse:0.346, l2:124.087
all zero rmse:0.507, rnn rmse:0.360, l2:124.087
all zero rmse:0.506, rnn rmse:0.369, l2:124.087
all zero rmse:0.506, rnn rmse:0.377, l2:124.087
all zero rmse:0.506, rnn rmse:0.384, l2:124.087
all zero rmse:0.506, rnn rmse:0.389, l2:124.087
all zero rmse:0.505, rnn rmse:0.392, l2:124.087
all zero rmse:0.505, rnn rmse:0.394, l2:124.087
all zero rmse:0.505, rnn rmse:0.395, l2:124.087
all zero rmse:0.505, rnn rmse:0.396, l2:124.087
all zero rmse:0.505, rnn rmse:0.396, l2:124.087
all zero rmse:0.505, rnn rmse:0.396, l2:124.087
all zero rmse:0.505, rnn rmse:0.395, l2:124.087
all zero rmse:0.505, rnn rmse:0.395, l2:124.087
all zero rmse:0.505, rnn rmse:0.394, l2:124.087
all zero rmse:0.505, rnn rmse:0.392, l2:124.087
all zero rmse:0.505, rnn rmse:0.391, l2:124.087
all zero rmse:0.505, rnn rmse:0.389, l2:124.087
all zero rmse:0.505, rnn rmse:0.387, l2:124.087
all zero rmse:0.505, rnn rmse:0.385, l2:124.087
all zero rmse:0.505, rnn rmse:0.382, l2:124.087
all zero rmse:0.506, rnn rmse:0.381, l2:124.087
all zero rmse:0.506, rnn rmse:0.378, l2:124.087
all zero rmse:0.506, rnn rmse:0.376, l2:124.087
all zero rmse:0.506, rnn rmse:0.373, l2:124.087
all zero rmse:0.506, rnn rmse:0.371, l2:124.087
all zero rmse:0.506, rnn rmse:0.368, l2:124.087
all zero rmse:0.506, rnn rmse:0.366, l2:124.087
all zero rmse:0.506, rnn rmse:0.361, l2:124.087
all zero rmse:0.506, rnn rmse:0.357, l2:124.087
all zero rmse:0.506, rnn rmse:0.352, l2:124.087
all zero rmse:0.506, rnn rmse:0.348, l2:124.087
all zero rmse:0.506, rnn rmse:0.344, l2:124.087
all zero rmse:0.506, rnn rmse:0.340, l2:124.087
all zero rmse:0.506, rnn rmse:0.336, l2:124.087
all zero rmse:0.506, rnn rmse:0.332, l2:124.087
all zero rmse:0.506, rnn rmse:0.328, l2:124.087
all zero rmse:0.506, rnn rmse:0.325, l2:124.087
all zero rmse:0.506, rnn rmse:0.321, l2:124.087
all zero rmse:0.506, rnn rmse:0.318, l2:124.087
all zero rmse:0.506, rnn rmse:0.314, l2:124.087
all zero rmse:0.506, rnn rmse:0.311, l2:124.087
all zero rmse:0.506, rnn rmse:0.308, l2:124.087
all zero rmse:0.506, rnn rmse:0.305, l2:124.087
all zero rmse:0.506, rnn rmse:0.302, l2:124.087
all zero rmse:0.506, rnn rmse:0.300, l2:124.087
all zero rmse:0.506, rnn rmse:0.297, l2:124.087
all zero rmse:0.506, rnn rmse:0.294, l2:124.087
all zero rmse:0.506, rnn rmse:0.292, l2:124.087
all zero rmse:0.506, rnn rmse:0.290, l2:124.087
all zero rmse:0.506, rnn rmse:0.289, l2:124.087
all zero rmse:0.506, rnn rmse:0.287, l2:124.087
all zero rmse:0.506, rnn rmse:0.286, l2:124.087
all zero rmse:0.506, rnn rmse:0.284, l2:124.087
all zero rmse:0.506, rnn rmse:0.283, l2:124.087
all zero rmse:0.506, rnn rmse:0.282, l2:124.087
all zero rmse:0.506, rnn rmse:0.281, l2:124.087
all zero rmse:0.506, rnn rmse:0.281, l2:124.087
all zero rmse:0.506, rnn rmse:0.281, l2:124.087
all zero rmse:0.506, rnn rmse:0.280, l2:124.087
train step 16 over
train step 17 over
train step 18 over
train step 19 over
train step 20 over
all zero rmse:0.507, rnn rmse:0.199, l2:127.788
all zero rmse:0.506, rnn rmse:0.245, l2:127.788
all zero rmse:0.506, rnn rmse:0.275, l2:127.788
all zero rmse:0.506, rnn rmse:0.299, l2:127.788
all zero rmse:0.506, rnn rmse:0.316, l2:127.788
all zero rmse:0.506, rnn rmse:0.327, l2:127.788
all zero rmse:0.506, rnn rmse:0.337, l2:127.788
all zero rmse:0.506, rnn rmse:0.345, l2:127.788
all zero rmse:0.506, rnn rmse:0.351, l2:127.788
all zero rmse:0.506, rnn rmse:0.356, l2:127.788
all zero rmse:0.506, rnn rmse:0.359, l2:127.788
all zero rmse:0.506, rnn rmse:0.362, l2:127.788
all zero rmse:0.506, rnn rmse:0.364, l2:127.788
all zero rmse:0.506, rnn rmse:0.366, l2:127.788
all zero rmse:0.506, rnn rmse:0.367, l2:127.788
all zero rmse:0.506, rnn rmse:0.367, l2:127.788
all zero rmse:0.506, rnn rmse:0.367, l2:127.788
all zero rmse:0.506, rnn rmse:0.366, l2:127.788
all zero rmse:0.506, rnn rmse:0.366, l2:127.788
all zero rmse:0.506, rnn rmse:0.365, l2:127.788
all zero rmse:0.506, rnn rmse:0.364, l2:127.788
all zero rmse:0.506, rnn rmse:0.363, l2:127.788
all zero rmse:0.506, rnn rmse:0.363, l2:127.788
all zero rmse:0.506, rnn rmse:0.361, l2:127.788
all zero rmse:0.506, rnn rmse:0.359, l2:127.788
all zero rmse:0.506, rnn rmse:0.358, l2:127.788
all zero rmse:0.506, rnn rmse:0.356, l2:127.788
all zero rmse:0.506, rnn rmse:0.354, l2:127.788
all zero rmse:0.506, rnn rmse:0.352, l2:127.788
all zero rmse:0.506, rnn rmse:0.350, l2:127.788
all zero rmse:0.506, rnn rmse:0.348, l2:127.788
all zero rmse:0.506, rnn rmse:0.347, l2:127.788
all zero rmse:0.506, rnn rmse:0.343, l2:127.788
all zero rmse:0.506, rnn rmse:0.339, l2:127.788
all zero rmse:0.506, rnn rmse:0.335, l2:127.788
all zero rmse:0.506, rnn rmse:0.331, l2:127.788
all zero rmse:0.506, rnn rmse:0.327, l2:127.788
all zero rmse:0.506, rnn rmse:0.323, l2:127.788
all zero rmse:0.506, rnn rmse:0.320, l2:127.788
all zero rmse:0.506, rnn rmse:0.316, l2:127.788
all zero rmse:0.506, rnn rmse:0.313, l2:127.788
all zero rmse:0.506, rnn rmse:0.310, l2:127.788
all zero rmse:0.506, rnn rmse:0.306, l2:127.788
all zero rmse:0.506, rnn rmse:0.303, l2:127.788
all zero rmse:0.506, rnn rmse:0.300, l2:127.788
all zero rmse:0.506, rnn rmse:0.297, l2:127.788
all zero rmse:0.506, rnn rmse:0.295, l2:127.788
all zero rmse:0.506, rnn rmse:0.292, l2:127.788
all zero rmse:0.506, rnn rmse:0.290, l2:127.788
all zero rmse:0.506, rnn rmse:0.288, l2:127.788
all zero rmse:0.506, rnn rmse:0.285, l2:127.788
all zero rmse:0.506, rnn rmse:0.283, l2:127.788
all zero rmse:0.506, rnn rmse:0.281, l2:127.788
all zero rmse:0.506, rnn rmse:0.279, l2:127.788
all zero rmse:0.506, rnn rmse:0.278, l2:127.788
all zero rmse:0.506, rnn rmse:0.277, l2:127.788
all zero rmse:0.506, rnn rmse:0.275, l2:127.788
all zero rmse:0.506, rnn rmse:0.274, l2:127.788
all zero rmse:0.506, rnn rmse:0.273, l2:127.788
all zero rmse:0.506, rnn rmse:0.273, l2:127.788
all zero rmse:0.506, rnn rmse:0.272, l2:127.788
all zero rmse:0.506, rnn rmse:0.272, l2:127.788
all zero rmse:0.506, rnn rmse:0.271, l2:127.788
all zero rmse:0.506, rnn rmse:0.271, l2:127.788
train step 21 over
train step 22 over
train step 23 over
train step 24 over
train step 25 over
all zero rmse:0.506, rnn rmse:0.195, l2:129.637
all zero rmse:0.507, rnn rmse:0.239, l2:129.637
all zero rmse:0.507, rnn rmse:0.266, l2:129.637
all zero rmse:0.506, rnn rmse:0.288, l2:129.637
all zero rmse:0.507, rnn rmse:0.303, l2:129.637
all zero rmse:0.507, rnn rmse:0.315, l2:129.637
all zero rmse:0.506, rnn rmse:0.323, l2:129.637
all zero rmse:0.506, rnn rmse:0.330, l2:129.637
all zero rmse:0.506, rnn rmse:0.336, l2:129.637
all zero rmse:0.506, rnn rmse:0.341, l2:129.637
WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/tpgr.py:569: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/tpgr.py:572: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/tpgr.py:573: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:From /home/clusterusers/ecavenaghi/RL_RS_Survey/TPGR/src/tpgr.py:612: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

all zero rmse:0.506, rnn rmse:0.344, l2:129.637
all zero rmse:0.506, rnn rmse:0.347, l2:129.637
all zero rmse:0.506, rnn rmse:0.349, l2:129.637
all zero rmse:0.506, rnn rmse:0.351, l2:129.637
all zero rmse:0.506, rnn rmse:0.351, l2:129.637
all zero rmse:0.505, rnn rmse:0.351, l2:129.637
all zero rmse:0.505, rnn rmse:0.351, l2:129.637
all zero rmse:0.505, rnn rmse:0.352, l2:129.637
all zero rmse:0.505, rnn rmse:0.351, l2:129.637
all zero rmse:0.505, rnn rmse:0.351, l2:129.637
all zero rmse:0.505, rnn rmse:0.350, l2:129.637
all zero rmse:0.505, rnn rmse:0.349, l2:129.637
all zero rmse:0.505, rnn rmse:0.348, l2:129.637
all zero rmse:0.505, rnn rmse:0.347, l2:129.637
all zero rmse:0.505, rnn rmse:0.346, l2:129.637
all zero rmse:0.505, rnn rmse:0.345, l2:129.637
all zero rmse:0.505, rnn rmse:0.343, l2:129.637
all zero rmse:0.505, rnn rmse:0.342, l2:129.637
all zero rmse:0.505, rnn rmse:0.340, l2:129.637
all zero rmse:0.505, rnn rmse:0.339, l2:129.637
all zero rmse:0.505, rnn rmse:0.337, l2:129.637
all zero rmse:0.505, rnn rmse:0.336, l2:129.637
all zero rmse:0.505, rnn rmse:0.332, l2:129.637
all zero rmse:0.505, rnn rmse:0.328, l2:129.637
all zero rmse:0.505, rnn rmse:0.325, l2:129.637
all zero rmse:0.505, rnn rmse:0.321, l2:129.637
all zero rmse:0.505, rnn rmse:0.318, l2:129.637
all zero rmse:0.505, rnn rmse:0.314, l2:129.637
all zero rmse:0.505, rnn rmse:0.311, l2:129.637
all zero rmse:0.505, rnn rmse:0.308, l2:129.637
all zero rmse:0.505, rnn rmse:0.305, l2:129.637
all zero rmse:0.505, rnn rmse:0.302, l2:129.637
all zero rmse:0.505, rnn rmse:0.299, l2:129.637
all zero rmse:0.505, rnn rmse:0.295, l2:129.637
all zero rmse:0.505, rnn rmse:0.293, l2:129.637
all zero rmse:0.505, rnn rmse:0.290, l2:129.637
all zero rmse:0.505, rnn rmse:0.287, l2:129.637
all zero rmse:0.505, rnn rmse:0.285, l2:129.637
all zero rmse:0.505, rnn rmse:0.283, l2:129.637
all zero rmse:0.505, rnn rmse:0.281, l2:129.637
all zero rmse:0.505, rnn rmse:0.278, l2:129.637
all zero rmse:0.505, rnn rmse:0.277, l2:129.637
all zero rmse:0.505, rnn rmse:0.275, l2:129.637
all zero rmse:0.505, rnn rmse:0.274, l2:129.637
all zero rmse:0.505, rnn rmse:0.272, l2:129.637
all zero rmse:0.505, rnn rmse:0.271, l2:129.637
all zero rmse:0.505, rnn rmse:0.270, l2:129.637
all zero rmse:0.505, rnn rmse:0.269, l2:129.637
all zero rmse:0.505, rnn rmse:0.268, l2:129.637
all zero rmse:0.505, rnn rmse:0.267, l2:129.637
all zero rmse:0.505, rnn rmse:0.267, l2:129.637
all zero rmse:0.505, rnn rmse:0.266, l2:129.637
all zero rmse:0.505, rnn rmse:0.266, l2:129.637
all zero rmse:0.505, rnn rmse:0.266, l2:129.637
train step 26 over
train step 27 over
train step 28 over
train step 29 over
train step 30 over
all zero rmse:0.508, rnn rmse:0.177, l2:132.335
all zero rmse:0.507, rnn rmse:0.217, l2:132.335
all zero rmse:0.506, rnn rmse:0.243, l2:132.335
all zero rmse:0.506, rnn rmse:0.262, l2:132.335
all zero rmse:0.506, rnn rmse:0.278, l2:132.335
all zero rmse:0.505, rnn rmse:0.289, l2:132.335
all zero rmse:0.506, rnn rmse:0.300, l2:132.335
all zero rmse:0.506, rnn rmse:0.308, l2:132.335
all zero rmse:0.505, rnn rmse:0.315, l2:132.335
all zero rmse:0.505, rnn rmse:0.321, l2:132.335
all zero rmse:0.505, rnn rmse:0.326, l2:132.335
all zero rmse:0.506, rnn rmse:0.330, l2:132.335
all zero rmse:0.506, rnn rmse:0.333, l2:132.335
all zero rmse:0.506, rnn rmse:0.336, l2:132.335
all zero rmse:0.506, rnn rmse:0.338, l2:132.335
all zero rmse:0.506, rnn rmse:0.339, l2:132.335
all zero rmse:0.506, rnn rmse:0.340, l2:132.335
all zero rmse:0.506, rnn rmse:0.341, l2:132.335
all zero rmse:0.506, rnn rmse:0.341, l2:132.335
all zero rmse:0.506, rnn rmse:0.341, l2:132.335
all zero rmse:0.506, rnn rmse:0.341, l2:132.335
all zero rmse:0.506, rnn rmse:0.341, l2:132.335
all zero rmse:0.506, rnn rmse:0.341, l2:132.335
all zero rmse:0.506, rnn rmse:0.340, l2:132.335
all zero rmse:0.506, rnn rmse:0.339, l2:132.335
all zero rmse:0.506, rnn rmse:0.338, l2:132.335
all zero rmse:0.506, rnn rmse:0.337, l2:132.335
all zero rmse:0.506, rnn rmse:0.336, l2:132.335
all zero rmse:0.506, rnn rmse:0.335, l2:132.335
all zero rmse:0.506, rnn rmse:0.334, l2:132.335
all zero rmse:0.506, rnn rmse:0.333, l2:132.335
all zero rmse:0.506, rnn rmse:0.331, l2:132.335
all zero rmse:0.506, rnn rmse:0.328, l2:132.335
all zero rmse:0.506, rnn rmse:0.325, l2:132.335
all zero rmse:0.506, rnn rmse:0.321, l2:132.335
all zero rmse:0.506, rnn rmse:0.318, l2:132.335
all zero rmse:0.506, rnn rmse:0.315, l2:132.335
all zero rmse:0.506, rnn rmse:0.311, l2:132.335
all zero rmse:0.506, rnn rmse:0.308, l2:132.335
all zero rmse:0.506, rnn rmse:0.305, l2:132.335
all zero rmse:0.506, rnn rmse:0.302, l2:132.335
all zero rmse:0.506, rnn rmse:0.299, l2:132.335
all zero rmse:0.506, rnn rmse:0.296, l2:132.335
all zero rmse:0.506, rnn rmse:0.293, l2:132.335
all zero rmse:0.506, rnn rmse:0.290, l2:132.335
all zero rmse:0.506, rnn rmse:0.287, l2:132.335
all zero rmse:0.506, rnn rmse:0.285, l2:132.335
all zero rmse:0.507, rnn rmse:0.283, l2:132.335
all zero rmse:0.507, rnn rmse:0.280, l2:132.335
all zero rmse:0.507, rnn rmse:0.278, l2:132.335
all zero rmse:0.506, rnn rmse:0.276, l2:132.335
all zero rmse:0.506, rnn rmse:0.274, l2:132.335
all zero rmse:0.506, rnn rmse:0.272, l2:132.335
all zero rmse:0.506, rnn rmse:0.271, l2:132.335
all zero rmse:0.506, rnn rmse:0.269, l2:132.335
all zero rmse:0.506, rnn rmse:0.268, l2:132.335
all zero rmse:0.506, rnn rmse:0.267, l2:132.335
all zero rmse:0.506, rnn rmse:0.266, l2:132.335
all zero rmse:0.507, rnn rmse:0.266, l2:132.335
all zero rmse:0.507, rnn rmse:0.265, l2:132.335
all zero rmse:0.507, rnn rmse:0.265, l2:132.335
all zero rmse:0.507, rnn rmse:0.264, l2:132.335
all zero rmse:0.506, rnn rmse:0.264, l2:132.335
all zero rmse:0.506, rnn rmse:0.264, l2:132.335
log: end pre-training
time: Fri Nov 26 23:23:31 2021
log: start constructing tree
time: Fri Nov 26 23:23:31 2021
leaf num count: 100
item num count: 100
log: end constructing tree
time: Fri Nov 26 23:23:32 2021
log: start training tpgr
time: Fri Nov 26 23:23:32 2021
log: get_aval completed
log: making graph
log: graph made
training step: 0
	train average reward over step: 0.1684, precision@32: 0.3615, recall@32: 0.3178, f1@32: 0.3121
	test  average reward over step: 0.2050, precision@32: 0.4128, recall@32: 0.3191, f1@32: 0.3378
	average rmse over train and test: 0.019396
log: evaluated

time: Fri Nov 26 23:23:48 2021
qs means: 0.34935
training step: 1
	train average reward over step: 0.2522, precision@32: 0.4445, recall@32: 0.4007, f1@32: 0.3896
	test  average reward over step: 0.2697, precision@32: 0.4766, recall@32: 0.3824, f1@32: 0.3975
	average rmse over train and test: 0.027664
log: evaluated

time: Fri Nov 26 23:25:05 2021
qs means: 0.44176
training step: 2
	train average reward over step: 0.2878, precision@32: 0.4874, recall@32: 0.4404, f1@32: 0.4303
	test  average reward over step: 0.3069, precision@32: 0.5262, recall@32: 0.4274, f1@32: 0.4433
	average rmse over train and test: 0.040667
log: evaluated

time: Fri Nov 26 23:26:20 2021
qs means: 0.49346
training step: 3
	train average reward over step: 0.3100, precision@32: 0.5134, recall@32: 0.4644, f1@32: 0.4548
	test  average reward over step: 0.3224, precision@32: 0.5450, recall@32: 0.4450, f1@32: 0.4614
	average rmse over train and test: 0.053270
log: evaluated

time: Fri Nov 26 23:27:34 2021
qs means: 0.53219
training step: 4
	train average reward over step: 0.3189, precision@32: 0.5241, recall@32: 0.4780, f1@32: 0.4662
	test  average reward over step: 0.3303, precision@32: 0.5594, recall@32: 0.4657, f1@32: 0.4766
	average rmse over train and test: 0.064499
log: evaluated

time: Fri Nov 26 23:28:49 2021
qs means: 0.59510
training step: 5
	train average reward over step: 0.3218, precision@32: 0.5302, recall@32: 0.4843, f1@32: 0.4726
	test  average reward over step: 0.3383, precision@32: 0.5675, recall@32: 0.4691, f1@32: 0.4831
	average rmse over train and test: 0.074530
log: evaluated

time: Fri Nov 26 23:30:04 2021
qs means: 0.60859
training step: 6
	train average reward over step: 0.3246, precision@32: 0.5309, recall@32: 0.4889, f1@32: 0.4742
	test  average reward over step: 0.3431, precision@32: 0.5709, recall@32: 0.4754, f1@32: 0.4875
	average rmse over train and test: 0.084138
log: evaluated

time: Fri Nov 26 23:31:20 2021
qs means: 0.64065
training step: 7
	train average reward over step: 0.3299, precision@32: 0.5368, recall@32: 0.4952, f1@32: 0.4802
	test  average reward over step: 0.3462, precision@32: 0.5744, recall@32: 0.4832, f1@32: 0.4917
	average rmse over train and test: 0.093164
log: evaluated

time: Fri Nov 26 23:32:35 2021
qs means: 0.68652
training step: 8
	train average reward over step: 0.3276, precision@32: 0.5364, recall@32: 0.4958, f1@32: 0.4804
	test  average reward over step: 0.3471, precision@32: 0.5763, recall@32: 0.4844, f1@32: 0.4935
	average rmse over train and test: 0.101433
log: evaluated

time: Fri Nov 26 23:33:50 2021
qs means: 0.69280
training step: 9
	train average reward over step: 0.3316, precision@32: 0.5398, recall@32: 0.5003, f1@32: 0.4841
	test  average reward over step: 0.3481, precision@32: 0.5747, recall@32: 0.4819, f1@32: 0.4916
	average rmse over train and test: 0.108585
log: evaluated

time: Fri Nov 26 23:35:05 2021
qs means: 0.69414
training step: 10
	train average reward over step: 0.3311, precision@32: 0.5398, recall@32: 0.5019, f1@32: 0.4844
	test  average reward over step: 0.3468, precision@32: 0.5747, recall@32: 0.4813, f1@32: 0.4920
	average rmse over train and test: 0.114217
log: evaluated

time: Fri Nov 26 23:36:21 2021
qs means: 0.71567
training step: 11
	train average reward over step: 0.3319, precision@32: 0.5400, recall@32: 0.5034, f1@32: 0.4847
	test  average reward over step: 0.3440, precision@32: 0.5709, recall@32: 0.4798, f1@32: 0.4890
	average rmse over train and test: 0.119178
log: evaluated

time: Fri Nov 26 23:37:36 2021
qs means: 0.72110
training step: 12
	train average reward over step: 0.3331, precision@32: 0.5428, recall@32: 0.5060, f1@32: 0.4874
	test  average reward over step: 0.3450, precision@32: 0.5697, recall@32: 0.4728, f1@32: 0.4858
	average rmse over train and test: 0.123438
log: evaluated

time: Fri Nov 26 23:38:52 2021
qs means: 0.72147
training step: 13
	train average reward over step: 0.3275, precision@32: 0.5377, recall@32: 0.4971, f1@32: 0.4817
	test  average reward over step: 0.3508, precision@32: 0.5750, recall@32: 0.4826, f1@32: 0.4929
	average rmse over train and test: 0.126809
log: evaluated

time: Fri Nov 26 23:40:07 2021
qs means: 0.72465
training step: 14
	train average reward over step: 0.3335, precision@32: 0.5402, recall@32: 0.4981, f1@32: 0.4847
	test  average reward over step: 0.3511, precision@32: 0.5775, recall@32: 0.4855, f1@32: 0.4951
	average rmse over train and test: 0.129831
log: evaluated

time: Fri Nov 26 23:41:22 2021
qs means: 0.73037
training step: 15
	train average reward over step: 0.3274, precision@32: 0.5374, recall@32: 0.4967, f1@32: 0.4818
	test  average reward over step: 0.3479, precision@32: 0.5731, recall@32: 0.4803, f1@32: 0.4909
	average rmse over train and test: 0.132194
log: evaluated

time: Fri Nov 26 23:42:38 2021
qs means: 0.72772
training step: 16
	train average reward over step: 0.3320, precision@32: 0.5416, recall@32: 0.5005, f1@32: 0.4862
	test  average reward over step: 0.3476, precision@32: 0.5753, recall@32: 0.4757, f1@32: 0.4909
	average rmse over train and test: 0.134633
log: evaluated

time: Fri Nov 26 23:43:53 2021
qs means: 0.73431
training step: 17
	train average reward over step: 0.3299, precision@32: 0.5395, recall@32: 0.5023, f1@32: 0.4841
	test  average reward over step: 0.3484, precision@32: 0.5753, recall@32: 0.4805, f1@32: 0.4925
	average rmse over train and test: 0.136619
log: evaluated

time: Fri Nov 26 23:45:09 2021
qs means: 0.73199
training step: 18
	train average reward over step: 0.3312, precision@32: 0.5398, recall@32: 0.5010, f1@32: 0.4849
	test  average reward over step: 0.3463, precision@32: 0.5694, recall@32: 0.4723, f1@32: 0.4864
	average rmse over train and test: 0.138576
log: evaluated

time: Fri Nov 26 23:46:24 2021
qs means: 0.73882
training step: 19
	train average reward over step: 0.3305, precision@32: 0.5377, recall@32: 0.4953, f1@32: 0.4823
	test  average reward over step: 0.3481, precision@32: 0.5766, recall@32: 0.4750, f1@32: 0.4913
	average rmse over train and test: 0.140261
log: evaluated

time: Fri Nov 26 23:47:39 2021
qs means: 0.74342
training step: 20
	train average reward over step: 0.3320, precision@32: 0.5400, recall@32: 0.4974, f1@32: 0.4839
	test  average reward over step: 0.3495, precision@32: 0.5744, recall@32: 0.4806, f1@32: 0.4911
	average rmse over train and test: 0.142354
log: evaluated

time: Fri Nov 26 23:48:54 2021
qs means: 0.74623
training step: 21
	train average reward over step: 0.3297, precision@32: 0.5378, recall@32: 0.4955, f1@32: 0.4816
	test  average reward over step: 0.3438, precision@32: 0.5716, recall@32: 0.4731, f1@32: 0.4873
	average rmse over train and test: 0.144348
log: evaluated

time: Fri Nov 26 23:50:10 2021
qs means: 0.74103
training step: 22
	train average reward over step: 0.3346, precision@32: 0.5409, recall@32: 0.5002, f1@32: 0.4856
	test  average reward over step: 0.3442, precision@32: 0.5678, recall@32: 0.4672, f1@32: 0.4824
	average rmse over train and test: 0.145954
log: evaluated

time: Fri Nov 26 23:51:25 2021
qs means: 0.74583
training step: 23
	train average reward over step: 0.3326, precision@32: 0.5398, recall@32: 0.4983, f1@32: 0.4842
	test  average reward over step: 0.3481, precision@32: 0.5734, recall@32: 0.4770, f1@32: 0.4899
	average rmse over train and test: 0.147349
log: evaluated

time: Fri Nov 26 23:52:40 2021
qs means: 0.77062
training step: 24
	train average reward over step: 0.3307, precision@32: 0.5395, recall@32: 0.4970, f1@32: 0.4838
	test  average reward over step: 0.3441, precision@32: 0.5681, recall@32: 0.4726, f1@32: 0.4848
	average rmse over train and test: 0.148561
log: evaluated

time: Fri Nov 26 23:53:55 2021
qs means: 0.74282
training step: 25
	train average reward over step: 0.3319, precision@32: 0.5404, recall@32: 0.4974, f1@32: 0.4841
	test  average reward over step: 0.3449, precision@32: 0.5713, recall@32: 0.4826, f1@32: 0.4886
	average rmse over train and test: 0.149899
log: evaluated

time: Fri Nov 26 23:55:11 2021
qs means: 0.74797
training step: 26
	train average reward over step: 0.3310, precision@32: 0.5366, recall@32: 0.4938, f1@32: 0.4808
	test  average reward over step: 0.3472, precision@32: 0.5737, recall@32: 0.4770, f1@32: 0.4891
	average rmse over train and test: 0.150769
log: evaluated

time: Fri Nov 26 23:56:26 2021
qs means: 0.75131
training step: 27
	train average reward over step: 0.3299, precision@32: 0.5393, recall@32: 0.4960, f1@32: 0.4834
	test  average reward over step: 0.3409, precision@32: 0.5666, recall@32: 0.4716, f1@32: 0.4839
	average rmse over train and test: 0.151749
log: evaluated

time: Fri Nov 26 23:57:41 2021
qs means: 0.73350
training step: 28
	train average reward over step: 0.3296, precision@32: 0.5359, recall@32: 0.4948, f1@32: 0.4797
	test  average reward over step: 0.3568, precision@32: 0.5800, recall@32: 0.4845, f1@32: 0.4961
	average rmse over train and test: 0.152510
log: evaluated

time: Fri Nov 26 23:58:57 2021
qs means: 0.73880
training step: 29
	train average reward over step: 0.3300, precision@32: 0.5395, recall@32: 0.4986, f1@32: 0.4832
	test  average reward over step: 0.3393, precision@32: 0.5659, recall@32: 0.4745, f1@32: 0.4842
	average rmse over train and test: 0.153187
log: evaluated

time: Sat Nov 27 00:00:12 2021
qs means: 0.74721
training step: 30
	train average reward over step: 0.3293, precision@32: 0.5358, recall@32: 0.4936, f1@32: 0.4799
	test  average reward over step: 0.3435, precision@32: 0.5675, recall@32: 0.4760, f1@32: 0.4853
	average rmse over train and test: 0.153813
log: evaluated

time: Sat Nov 27 00:01:27 2021
qs means: 0.75690
training step: 31
	train average reward over step: 0.3280, precision@32: 0.5356, recall@32: 0.4956, f1@32: 0.4803
	test  average reward over step: 0.3497, precision@32: 0.5753, recall@32: 0.4845, f1@32: 0.4921
	average rmse over train and test: 0.154434
log: evaluated

time: Sat Nov 27 00:02:42 2021
qs means: 0.75102
training step: 32
	train average reward over step: 0.3295, precision@32: 0.5362, recall@32: 0.4955, f1@32: 0.4810
	test  average reward over step: 0.3532, precision@32: 0.5756, recall@32: 0.4848, f1@32: 0.4936
	average rmse over train and test: 0.154892
log: evaluated

time: Sat Nov 27 00:03:58 2021
qs means: 0.76163
training step: 33
	train average reward over step: 0.3269, precision@32: 0.5336, recall@32: 0.4926, f1@32: 0.4773
	test  average reward over step: 0.3406, precision@32: 0.5644, recall@32: 0.4683, f1@32: 0.4813
	average rmse over train and test: 0.155315
log: evaluated

time: Sat Nov 27 00:05:13 2021
qs means: 0.74058
training step: 34
	train average reward over step: 0.3315, precision@32: 0.5377, recall@32: 0.4965, f1@32: 0.4823
	test  average reward over step: 0.3468, precision@32: 0.5706, recall@32: 0.4771, f1@32: 0.4872
	average rmse over train and test: 0.156022
log: evaluated

time: Sat Nov 27 00:06:28 2021
qs means: 0.75971
training step: 35
	train average reward over step: 0.3254, precision@32: 0.5284, recall@32: 0.4880, f1@32: 0.4730
	test  average reward over step: 0.3428, precision@32: 0.5644, recall@32: 0.4679, f1@32: 0.4806
	average rmse over train and test: 0.156076
log: evaluated

time: Sat Nov 27 00:07:43 2021
qs means: 0.75376
training step: 36
	train average reward over step: 0.3284, precision@32: 0.5329, recall@32: 0.4900, f1@32: 0.4765
	test  average reward over step: 0.3441, precision@32: 0.5734, recall@32: 0.4764, f1@32: 0.4898
	average rmse over train and test: 0.157007
log: evaluated

time: Sat Nov 27 00:08:59 2021
qs means: 0.76052
training step: 37
	train average reward over step: 0.3298, precision@32: 0.5344, recall@32: 0.4917, f1@32: 0.4783
	test  average reward over step: 0.3524, precision@32: 0.5759, recall@32: 0.4762, f1@32: 0.4911
	average rmse over train and test: 0.157577
log: evaluated

time: Sat Nov 27 00:10:14 2021
qs means: 0.74654
training step: 38
	train average reward over step: 0.3307, precision@32: 0.5362, recall@32: 0.4888, f1@32: 0.4794
	test  average reward over step: 0.3412, precision@32: 0.5647, recall@32: 0.4722, f1@32: 0.4832
	average rmse over train and test: 0.157783
log: evaluated

time: Sat Nov 27 00:11:29 2021
qs means: 0.74743
training step: 39
	train average reward over step: 0.3279, precision@32: 0.5341, recall@32: 0.4903, f1@32: 0.4778
	test  average reward over step: 0.3532, precision@32: 0.5766, recall@32: 0.4779, f1@32: 0.4921
	average rmse over train and test: 0.158479
log: evaluated

time: Sat Nov 27 00:12:44 2021
qs means: 0.77637
training step: 40
	train average reward over step: 0.3274, precision@32: 0.5325, recall@32: 0.4904, f1@32: 0.4764
	test  average reward over step: 0.3438, precision@32: 0.5669, recall@32: 0.4773, f1@32: 0.4853
	average rmse over train and test: 0.158339
log: evaluated

time: Sat Nov 27 00:14:00 2021
qs means: 0.75963
training step: 41
	train average reward over step: 0.3324, precision@32: 0.5402, recall@32: 0.4977, f1@32: 0.4839
	test  average reward over step: 0.3434, precision@32: 0.5644, recall@32: 0.4677, f1@32: 0.4813
	average rmse over train and test: 0.159058
log: evaluated

time: Sat Nov 27 00:15:15 2021
qs means: 0.75762
training step: 42
	train average reward over step: 0.3286, precision@32: 0.5365, recall@32: 0.4923, f1@32: 0.4798
	test  average reward over step: 0.3494, precision@32: 0.5694, recall@32: 0.4714, f1@32: 0.4852
	average rmse over train and test: 0.159213
log: evaluated

time: Sat Nov 27 00:16:30 2021
qs means: 0.74871
training step: 43
	train average reward over step: 0.3291, precision@32: 0.5346, recall@32: 0.4948, f1@32: 0.4794
	test  average reward over step: 0.3333, precision@32: 0.5541, recall@32: 0.4564, f1@32: 0.4711
	average rmse over train and test: 0.159513
log: evaluated

time: Sat Nov 27 00:17:45 2021
qs means: 0.76900
training step: 44
	train average reward over step: 0.3318, precision@32: 0.5373, recall@32: 0.4944, f1@32: 0.4810
	test  average reward over step: 0.3353, precision@32: 0.5606, recall@32: 0.4663, f1@32: 0.4786
	average rmse over train and test: 0.159886
log: evaluated

time: Sat Nov 27 00:19:00 2021
qs means: 0.76599
training step: 45
	train average reward over step: 0.3299, precision@32: 0.5355, recall@32: 0.4915, f1@32: 0.4790
	test  average reward over step: 0.3486, precision@32: 0.5706, recall@32: 0.4825, f1@32: 0.4888
	average rmse over train and test: 0.160150
log: evaluated

time: Sat Nov 27 00:20:15 2021
qs means: 0.76425
training step: 46
	train average reward over step: 0.3308, precision@32: 0.5354, recall@32: 0.4925, f1@32: 0.4794
	test  average reward over step: 0.3376, precision@32: 0.5597, recall@32: 0.4663, f1@32: 0.4779
	average rmse over train and test: 0.160497
log: evaluated

time: Sat Nov 27 00:21:30 2021
qs means: 0.78012
training step: 47
	train average reward over step: 0.3292, precision@32: 0.5357, recall@32: 0.4924, f1@32: 0.4794
	test  average reward over step: 0.3369, precision@32: 0.5591, recall@32: 0.4671, f1@32: 0.4767
	average rmse over train and test: 0.161040
log: evaluated

time: Sat Nov 27 00:22:46 2021
qs means: 0.75187
training step: 48
	train average reward over step: 0.3337, precision@32: 0.5378, recall@32: 0.4946, f1@32: 0.4820
	test  average reward over step: 0.3436, precision@32: 0.5644, recall@32: 0.4754, f1@32: 0.4828
	average rmse over train and test: 0.160835
log: evaluated

time: Sat Nov 27 00:24:01 2021
qs means: 0.75288
training step: 49
	train average reward over step: 0.3267, precision@32: 0.5315, recall@32: 0.4872, f1@32: 0.4749
	test  average reward over step: 0.3432, precision@32: 0.5628, recall@32: 0.4726, f1@32: 0.4812
	average rmse over train and test: 0.161244
log: evaluated

time: Sat Nov 27 00:25:16 2021
qs means: 0.75390
training step: 50
	train average reward over step: 0.3263, precision@32: 0.5319, recall@32: 0.4914, f1@32: 0.4767
	test  average reward over step: 0.3459, precision@32: 0.5653, recall@32: 0.4752, f1@32: 0.4821
	average rmse over train and test: 0.161591
log: evaluated

time: Sat Nov 27 00:26:31 2021
qs means: 0.75298
training step: 51
	train average reward over step: 0.3283, precision@32: 0.5330, recall@32: 0.4924, f1@32: 0.4776
	test  average reward over step: 0.3421, precision@32: 0.5594, recall@32: 0.4633, f1@32: 0.4759
	average rmse over train and test: 0.161827
log: evaluated

time: Sat Nov 27 00:27:47 2021
qs means: 0.76077
training step: 52
	train average reward over step: 0.3288, precision@32: 0.5345, recall@32: 0.4883, f1@32: 0.4772
	test  average reward over step: 0.3514, precision@32: 0.5728, recall@32: 0.4828, f1@32: 0.4906
	average rmse over train and test: 0.162236
log: evaluated

time: Sat Nov 27 00:29:02 2021
qs means: 0.74256
training step: 53
	train average reward over step: 0.3265, precision@32: 0.5322, recall@32: 0.4895, f1@32: 0.4765
	test  average reward over step: 0.3478, precision@32: 0.5691, recall@32: 0.4734, f1@32: 0.4852
	average rmse over train and test: 0.162304
log: evaluated

time: Sat Nov 27 00:30:17 2021
qs means: 0.76637
training step: 54
	train average reward over step: 0.3277, precision@32: 0.5340, recall@32: 0.4920, f1@32: 0.4781
	test  average reward over step: 0.3481, precision@32: 0.5687, recall@32: 0.4800, f1@32: 0.4873
	average rmse over train and test: 0.162458
log: evaluated

time: Sat Nov 27 00:31:32 2021
qs means: 0.75676
training step: 55
	train average reward over step: 0.3290, precision@32: 0.5335, recall@32: 0.4911, f1@32: 0.4773
	test  average reward over step: 0.3458, precision@32: 0.5672, recall@32: 0.4781, f1@32: 0.4847
	average rmse over train and test: 0.163034
log: evaluated

time: Sat Nov 27 00:32:47 2021
qs means: 0.76192
training step: 56
	train average reward over step: 0.3252, precision@32: 0.5305, recall@32: 0.4876, f1@32: 0.4740
	test  average reward over step: 0.3465, precision@32: 0.5659, recall@32: 0.4719, f1@32: 0.4818
	average rmse over train and test: 0.162924
log: evaluated

time: Sat Nov 27 00:34:03 2021
qs means: 0.74084
training step: 57
	train average reward over step: 0.3253, precision@32: 0.5305, recall@32: 0.4874, f1@32: 0.4746
	test  average reward over step: 0.3531, precision@32: 0.5781, recall@32: 0.4800, f1@32: 0.4937
	average rmse over train and test: 0.163838
log: evaluated

time: Sat Nov 27 00:35:18 2021
qs means: 0.75809
training step: 58
	train average reward over step: 0.3228, precision@32: 0.5246, recall@32: 0.4840, f1@32: 0.4691
	test  average reward over step: 0.3466, precision@32: 0.5681, recall@32: 0.4716, f1@32: 0.4850
	average rmse over train and test: 0.163845
log: evaluated

time: Sat Nov 27 00:36:33 2021
qs means: 0.75186
training step: 59
	train average reward over step: 0.3315, precision@32: 0.5356, recall@32: 0.4941, f1@32: 0.4798
	test  average reward over step: 0.3451, precision@32: 0.5675, recall@32: 0.4671, f1@32: 0.4814
	average rmse over train and test: 0.163923
log: evaluated

time: Sat Nov 27 00:37:48 2021
qs means: 0.75570
training step: 60
	train average reward over step: 0.3300, precision@32: 0.5343, recall@32: 0.4933, f1@32: 0.4787
	test  average reward over step: 0.3506, precision@32: 0.5759, recall@32: 0.4830, f1@32: 0.4927
	average rmse over train and test: 0.164316
log: evaluated

time: Sat Nov 27 00:39:04 2021
qs means: 0.74553
training step: 61
	train average reward over step: 0.3298, precision@32: 0.5367, recall@32: 0.4936, f1@32: 0.4801
	test  average reward over step: 0.3456, precision@32: 0.5675, recall@32: 0.4684, f1@32: 0.4827
	average rmse over train and test: 0.164649
log: evaluated

time: Sat Nov 27 00:40:19 2021
qs means: 0.75272
training step: 62
	train average reward over step: 0.3274, precision@32: 0.5292, recall@32: 0.4835, f1@32: 0.4729
	test  average reward over step: 0.3482, precision@32: 0.5666, recall@32: 0.4743, f1@32: 0.4850
	average rmse over train and test: 0.164671
log: evaluated

time: Sat Nov 27 00:41:34 2021
qs means: 0.75426
training step: 63
	train average reward over step: 0.3264, precision@32: 0.5318, recall@32: 0.4870, f1@32: 0.4749
	test  average reward over step: 0.3494, precision@32: 0.5750, recall@32: 0.4838, f1@32: 0.4921
	average rmse over train and test: 0.165053
log: evaluated

time: Sat Nov 27 00:42:49 2021
qs means: 0.75885
training step: 64
	train average reward over step: 0.3303, precision@32: 0.5358, recall@32: 0.4952, f1@32: 0.4799
	test  average reward over step: 0.3399, precision@32: 0.5578, recall@32: 0.4660, f1@32: 0.4750
	average rmse over train and test: 0.165392
log: evaluated

time: Sat Nov 27 00:44:04 2021
qs means: 0.74667
training step: 65
	train average reward over step: 0.3242, precision@32: 0.5292, recall@32: 0.4848, f1@32: 0.4728
	test  average reward over step: 0.3430, precision@32: 0.5681, recall@32: 0.4665, f1@32: 0.4826
	average rmse over train and test: 0.165525
log: evaluated

time: Sat Nov 27 00:45:19 2021
qs means: 0.76822
training step: 66
	train average reward over step: 0.3262, precision@32: 0.5338, recall@32: 0.4871, f1@32: 0.4771
	test  average reward over step: 0.3386, precision@32: 0.5597, recall@32: 0.4571, f1@32: 0.4739
	average rmse over train and test: 0.165774
log: evaluated

time: Sat Nov 27 00:46:34 2021
qs means: 0.76029
training step: 67
	train average reward over step: 0.3249, precision@32: 0.5295, recall@32: 0.4844, f1@32: 0.4736
	test  average reward over step: 0.3463, precision@32: 0.5634, recall@32: 0.4625, f1@32: 0.4779
	average rmse over train and test: 0.166560
log: evaluated

time: Sat Nov 27 00:47:49 2021
qs means: 0.75223
training step: 68
	train average reward over step: 0.3286, precision@32: 0.5341, recall@32: 0.4917, f1@32: 0.4787
	test  average reward over step: 0.3404, precision@32: 0.5550, recall@32: 0.4650, f1@32: 0.4734
	average rmse over train and test: 0.166673
log: evaluated

time: Sat Nov 27 00:49:05 2021
qs means: 0.77341
training step: 69
	train average reward over step: 0.3250, precision@32: 0.5285, recall@32: 0.4847, f1@32: 0.4721
	test  average reward over step: 0.3444, precision@32: 0.5675, recall@32: 0.4659, f1@32: 0.4822
	average rmse over train and test: 0.167135
log: evaluated

time: Sat Nov 27 00:50:20 2021
qs means: 0.75474
training step: 70
	train average reward over step: 0.3265, precision@32: 0.5314, recall@32: 0.4850, f1@32: 0.4749
	test  average reward over step: 0.3433, precision@32: 0.5637, recall@32: 0.4657, f1@32: 0.4801
	average rmse over train and test: 0.167720
log: evaluated

time: Sat Nov 27 00:51:35 2021
qs means: 0.76224
training step: 71
	train average reward over step: 0.3292, precision@32: 0.5347, recall@32: 0.4898, f1@32: 0.4783
	test  average reward over step: 0.3453, precision@32: 0.5650, recall@32: 0.4698, f1@32: 0.4828
	average rmse over train and test: 0.168176
log: evaluated

time: Sat Nov 27 00:52:50 2021
qs means: 0.76645
training step: 72
	train average reward over step: 0.3245, precision@32: 0.5280, recall@32: 0.4836, f1@32: 0.4714
	test  average reward over step: 0.3469, precision@32: 0.5678, recall@32: 0.4741, f1@32: 0.4841
	average rmse over train and test: 0.168843
log: evaluated

time: Sat Nov 27 00:54:05 2021
qs means: 0.75062
training step: 73
	train average reward over step: 0.3290, precision@32: 0.5341, recall@32: 0.4900, f1@32: 0.4777
	test  average reward over step: 0.3398, precision@32: 0.5575, recall@32: 0.4545, f1@32: 0.4725
	average rmse over train and test: 0.169066
log: evaluated

time: Sat Nov 27 00:55:20 2021
qs means: 0.73728
training step: 74
	train average reward over step: 0.3311, precision@32: 0.5388, recall@32: 0.4957, f1@32: 0.4825
	test  average reward over step: 0.3393, precision@32: 0.5606, recall@32: 0.4681, f1@32: 0.4775
	average rmse over train and test: 0.169962
log: evaluated

time: Sat Nov 27 00:56:35 2021
qs means: 0.74180
training step: 75
	train average reward over step: 0.3277, precision@32: 0.5325, recall@32: 0.4854, f1@32: 0.4750
	test  average reward over step: 0.3386, precision@32: 0.5606, recall@32: 0.4655, f1@32: 0.4781
	average rmse over train and test: 0.170575
log: evaluated

time: Sat Nov 27 00:57:50 2021
qs means: 0.76564
training step: 76
	train average reward over step: 0.3293, precision@32: 0.5358, recall@32: 0.4930, f1@32: 0.4792
	test  average reward over step: 0.3478, precision@32: 0.5691, recall@32: 0.4802, f1@32: 0.4873
	average rmse over train and test: 0.171243
log: evaluated

time: Sat Nov 27 00:59:05 2021
qs means: 0.75561
training step: 77
	train average reward over step: 0.3281, precision@32: 0.5358, recall@32: 0.4915, f1@32: 0.4792
	test  average reward over step: 0.3464, precision@32: 0.5691, recall@32: 0.4788, f1@32: 0.4873
	average rmse over train and test: 0.172074
log: evaluated

time: Sat Nov 27 01:00:20 2021
qs means: 0.74528
training step: 78
	train average reward over step: 0.3245, precision@32: 0.5288, recall@32: 0.4800, f1@32: 0.4716
	test  average reward over step: 0.3388, precision@32: 0.5581, recall@32: 0.4641, f1@32: 0.4746
	average rmse over train and test: 0.172512
log: evaluated

time: Sat Nov 27 01:01:35 2021
qs means: 0.75303
training step: 79
	train average reward over step: 0.3290, precision@32: 0.5347, recall@32: 0.4919, f1@32: 0.4781
	test  average reward over step: 0.3374, precision@32: 0.5547, recall@32: 0.4598, f1@32: 0.4708
	average rmse over train and test: 0.173114
log: evaluated

time: Sat Nov 27 01:02:50 2021
qs means: 0.75764
training step: 80
	train average reward over step: 0.3299, precision@32: 0.5357, recall@32: 0.4942, f1@32: 0.4794
	test  average reward over step: 0.3481, precision@32: 0.5659, recall@32: 0.4768, f1@32: 0.4842
	average rmse over train and test: 0.173935
log: evaluated

time: Sat Nov 27 01:04:05 2021
qs means: 0.75549
training step: 81
	train average reward over step: 0.3260, precision@32: 0.5312, recall@32: 0.4883, f1@32: 0.4750
	test  average reward over step: 0.3506, precision@32: 0.5734, recall@32: 0.4773, f1@32: 0.4892
	average rmse over train and test: 0.174708
log: evaluated

time: Sat Nov 27 01:05:21 2021
qs means: 0.76354
training step: 82
	train average reward over step: 0.3256, precision@32: 0.5307, recall@32: 0.4855, f1@32: 0.4746
	test  average reward over step: 0.3356, precision@32: 0.5569, recall@32: 0.4627, f1@32: 0.4748
	average rmse over train and test: 0.174887
log: evaluated

time: Sat Nov 27 01:06:35 2021
qs means: 0.77997
training step: 83
	train average reward over step: 0.3283, precision@32: 0.5332, recall@32: 0.4900, f1@32: 0.4772
	test  average reward over step: 0.3466, precision@32: 0.5706, recall@32: 0.4716, f1@32: 0.4848
	average rmse over train and test: 0.175341
log: evaluated

time: Sat Nov 27 01:07:51 2021
qs means: 0.77286
training step: 84
	train average reward over step: 0.3265, precision@32: 0.5297, recall@32: 0.4806, f1@32: 0.4719
	test  average reward over step: 0.3456, precision@32: 0.5694, recall@32: 0.4804, f1@32: 0.4878
	average rmse over train and test: 0.175866
log: evaluated

time: Sat Nov 27 01:09:06 2021
qs means: 0.76329
training step: 85
	train average reward over step: 0.3281, precision@32: 0.5346, recall@32: 0.4898, f1@32: 0.4784
	test  average reward over step: 0.3410, precision@32: 0.5619, recall@32: 0.4694, f1@32: 0.4795
	average rmse over train and test: 0.176417
log: evaluated

time: Sat Nov 27 01:10:21 2021
qs means: 0.75350
training step: 86
	train average reward over step: 0.3294, precision@32: 0.5337, recall@32: 0.4888, f1@32: 0.4766
	test  average reward over step: 0.3462, precision@32: 0.5675, recall@32: 0.4683, f1@32: 0.4830
	average rmse over train and test: 0.176463
log: evaluated

time: Sat Nov 27 01:11:37 2021
qs means: 0.75623
training step: 87
	train average reward over step: 0.3266, precision@32: 0.5298, recall@32: 0.4888, f1@32: 0.4741
	test  average reward over step: 0.3493, precision@32: 0.5687, recall@32: 0.4728, f1@32: 0.4851
	average rmse over train and test: 0.176854
log: evaluated

time: Sat Nov 27 01:12:51 2021
qs means: 0.76548
training step: 88
	train average reward over step: 0.3291, precision@32: 0.5339, recall@32: 0.4901, f1@32: 0.4777
	test  average reward over step: 0.3454, precision@32: 0.5666, recall@32: 0.4758, f1@32: 0.4841
	average rmse over train and test: 0.177503
log: evaluated

time: Sat Nov 27 01:14:06 2021
qs means: 0.76709
training step: 89
	train average reward over step: 0.3285, precision@32: 0.5322, recall@32: 0.4893, f1@32: 0.4765
	test  average reward over step: 0.3488, precision@32: 0.5684, recall@32: 0.4698, f1@32: 0.4832
	average rmse over train and test: 0.177823
log: evaluated

time: Sat Nov 27 01:15:22 2021
qs means: 0.73728
training step: 90
	train average reward over step: 0.3337, precision@32: 0.5398, recall@32: 0.4969, f1@32: 0.4833
	test  average reward over step: 0.3434, precision@32: 0.5606, recall@32: 0.4645, f1@32: 0.4760
	average rmse over train and test: 0.178098
log: evaluated

time: Sat Nov 27 01:16:37 2021
qs means: 0.75764
training step: 91
	train average reward over step: 0.3319, precision@32: 0.5362, recall@32: 0.4920, f1@32: 0.4793
	test  average reward over step: 0.3523, precision@32: 0.5728, recall@32: 0.4774, f1@32: 0.4889
	average rmse over train and test: 0.178572
log: evaluated

time: Sat Nov 27 01:17:52 2021
qs means: 0.74522
training step: 92
	train average reward over step: 0.3285, precision@32: 0.5326, recall@32: 0.4868, f1@32: 0.4762
	test  average reward over step: 0.3432, precision@32: 0.5631, recall@32: 0.4666, f1@32: 0.4790
	average rmse over train and test: 0.178912
log: evaluated

time: Sat Nov 27 01:19:07 2021
qs means: 0.76352
training step: 93
	train average reward over step: 0.3262, precision@32: 0.5299, recall@32: 0.4846, f1@32: 0.4730
	test  average reward over step: 0.3394, precision@32: 0.5569, recall@32: 0.4608, f1@32: 0.4722
	average rmse over train and test: 0.179175
log: evaluated

time: Sat Nov 27 01:20:22 2021
qs means: 0.75540
training step: 94
	train average reward over step: 0.3271, precision@32: 0.5319, recall@32: 0.4885, f1@32: 0.4750
	test  average reward over step: 0.3412, precision@32: 0.5597, recall@32: 0.4631, f1@32: 0.4763
	average rmse over train and test: 0.179592
log: evaluated

time: Sat Nov 27 01:21:37 2021
qs means: 0.76515
training step: 95
	train average reward over step: 0.3250, precision@32: 0.5311, recall@32: 0.4885, f1@32: 0.4749
	test  average reward over step: 0.3406, precision@32: 0.5644, recall@32: 0.4694, f1@32: 0.4804
	average rmse over train and test: 0.179794
log: evaluated

time: Sat Nov 27 01:22:52 2021
qs means: 0.75949
training step: 96
	train average reward over step: 0.3226, precision@32: 0.5263, recall@32: 0.4854, f1@32: 0.4702
	test  average reward over step: 0.3493, precision@32: 0.5697, recall@32: 0.4705, f1@32: 0.4853
	average rmse over train and test: 0.180011
log: evaluated

time: Sat Nov 27 01:24:07 2021
qs means: 0.75340
training step: 97
	train average reward over step: 0.3335, precision@32: 0.5399, recall@32: 0.4932, f1@32: 0.4832
	test  average reward over step: 0.3543, precision@32: 0.5769, recall@32: 0.4815, f1@32: 0.4919
	average rmse over train and test: 0.180422
log: evaluated

time: Sat Nov 27 01:25:23 2021
qs means: 0.76123
training step: 98
	train average reward over step: 0.3246, precision@32: 0.5291, recall@32: 0.4833, f1@32: 0.4722
	test  average reward over step: 0.3478, precision@32: 0.5650, recall@32: 0.4659, f1@32: 0.4817
	average rmse over train and test: 0.180541
log: evaluated

time: Sat Nov 27 01:26:38 2021
qs means: 0.74006
training step: 99
	train average reward over step: 0.3299, precision@32: 0.5323, recall@32: 0.4906, f1@32: 0.4759
	test  average reward over step: 0.3516, precision@32: 0.5750, recall@32: 0.4776, f1@32: 0.4890
	average rmse over train and test: 0.180602
log: evaluated

time: Sat Nov 27 01:27:53 2021
qs means: 0.75767
log: end training tpgr
